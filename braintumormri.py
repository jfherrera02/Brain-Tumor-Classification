# -*- coding: utf-8 -*-
"""BrainTumorMRI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1itgECSqKXzDHyuxcjMmwIcFlfHBpf9tv

# **Brain Tumor MRI Image Classification Using CNN**
---
"""

# Commented out IPython magic to ensure Python compatibility.
from sklearn.model_selection import train_test_split
#Loading the images dataset
import os
import cv2
import numpy as np

# Load the TensorBoard notebook extension
# %load_ext tensorboard
import datetime

import kagglehub

# Download the dataset and get the path
path = kagglehub.dataset_download("rm1000/brain-tumor-mri-scans")

# Print the path where files are saved
print("Path to dataset files:", path)

import os
# verify the directories are valid
print("Dataset files:")
print(os.listdir(path))

"""# **Load the Data and categorize them while keeping track of their counts**"""

import cv2
import numpy as np

images = []
labels = []

from collections import defaultdict

# class labels/ amount of images can be counted with this variable
label_counts = defaultdict(int)

# Class names listed here
class_names = sorted(os.listdir(path))  # Ensures alphabetical order

for class_name in class_names:
    class_path = os.path.join(path, class_name)
    if os.path.isdir(class_path):  # Only process directories
        for image_file in os.listdir(class_path):
            #if label_counts[class_name] >= 1200:
              #break  # Stop adding images for this class
            image_path = os.path.join(class_path, image_file)
            try:
                # Read the image
                img = cv2.imread(image_path)
                if img is not None:
                    # Resize the image to 224x224 to use with ResNet
                    img_resized = cv2.resize(img, (224, 224))
                    images.append(img_resized)
                    labels.append(class_name)
                    # Update the class image counts here for later use
                    label_counts[class_name] += 1

                else:
                    print(f"Warning: Unable to load {image_path}")
            except Exception as e:
                print(f"Error processing {image_path}: {e}")

# Convert to NumPy arrays
images = np.array(images)
labels = np.array(labels)
# Print the amount of images per class
for class_name, count in label_counts.items():
    print(f"{class_name}: {count}")

import matplotlib.pyplot as plt
# Here we can visualize the distribution with a pie chart
# Extract class names and counts
class_names = list(label_counts.keys())
counts = list(label_counts.values())

# Plotting the pie chart
plt.figure(figsize=(8, 8))
plt.pie(counts, labels=class_names, autopct='%1.1f%%', startangle=140, textprops={'fontsize': 12})
plt.title("Percentage per Class", fontsize=14)
plt.show()

"""# **Separate Images into Training, Validation, and Testing**"""

# Create the training set and separate 30% for test/val for a 70-15-15 split
train_images, temp_images, train_labels, temp_labels = train_test_split(
    images, labels, test_size=0.3, random_state=42, stratify= labels)

# Create the validation set and separate 50% for test for a 70-15-15 split
val_images, test_images, val_labels, test_labels = train_test_split(
    temp_images, temp_labels, test_size=0.5, random_state=42, stratify= temp_labels)

# Print the amount of images per data set with a 70-15-15 split
print(f"Training images: {len(train_images)}")
print(f"Validation images: {len(val_images)}")
print(f"Test images: {len(test_images)}")

import tensorflow as tf
from sklearn.preprocessing import LabelEncoder

# Normalize the images
train_images = train_images.astype('float32') / 255.0
val_images = val_images.astype('float32') / 255.0
test_images = test_images.astype('float32') / 255.0

# Initialize LabelEncoder
label_encoder = LabelEncoder()

# Fit and transform labels to numerical values
train_labels_encoded = label_encoder.fit_transform(train_labels)
val_labels_encoded = label_encoder.transform(val_labels)
test_labels_encoded = label_encoder.transform(test_labels)

#Initialize batch size
BATCH_SIZE = 32

# Create TensorFlow datasets with encoded labels
train_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels_encoded))
# Also shuffle the datasets before batching
train_ds = train_ds.shuffle(buffer_size=len(train_images)).batch(BATCH_SIZE)

val_ds = tf.data.Dataset.from_tensor_slices((val_images, val_labels_encoded))
val_ds = val_ds.shuffle(buffer_size=len(val_images)).batch(BATCH_SIZE)

test_ds = tf.data.Dataset.from_tensor_slices((test_images, test_labels_encoded))
test_ds = test_ds.shuffle(buffer_size=len(test_images)).batch(BATCH_SIZE)

# Optimize dataset performance
train_ds = train_ds.cache().prefetch(tf.data.AUTOTUNE)
val_ds = val_ds.cache().prefetch(tf.data.AUTOTUNE)
test_ds = test_ds.cache().prefetch(tf.data.AUTOTUNE)

from collections import Counter

# Check class distributions in split datasets
print("Class distribution in training set:", Counter(train_labels))
print("Class distribution in validation set:", Counter(val_labels))
print("Class distribution in test set:", Counter(test_labels))

"""# **Build the model using ResNet as the base**"""

# from tensorflow.keras.applications import ResNet50
from tensorflow.keras import layers, models
from tensorflow.keras.applications import ResNet50

# Load ResNet50 base model
base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Set to True for backpropagation
base_model.trainable = True #changed to true

# Add custom layers
model = models.Sequential([
    base_model, # resnet
    layers.GlobalAveragePooling2D(), # Average all values
    layers.Dense(128, activation='relu'),  # Hidden layer
    layers.Dropout(0.5),  # Dropout layer to reduce overfitting
    layers.Dense(len(class_names), activation='softmax')  # Output layer
])

# Compile the model
# adam has a default learning rate of '0.001' but I chose to use 0.0005 (5e-4) since I am using ResNet50 base_model.trainable = True
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
#odel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.summary()  # Print a summary of the model architecture

# Commented out IPython magic to ensure Python compatibility.
# Set up TensorBoard callback
log_dir = "logs/fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)

from tensorflow.keras.callbacks import ReduceLROnPlateau
# Define the learning rate scheduler, before this validation would severely underperform
lr_reducer = ReduceLROnPlateau(
    monitor='val_loss', # val_loss had evident issues so we monitor it
    factor=0.5, # reduce the learning rate by 50%
    patience=2,# Waits 2 epochs for val to improve
    verbose=1, # Enables messaging updates
    min_lr=1e-6 # Minimum learning rate
)

# Train the model with TensorBoard monitoring
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=20,
    callbacks=[tensorboard_callback, lr_reducer] # tensorboard and learning rate scheduler for validation ds

)

# %tensorboard --logdir logs/fit

# Print the results of our training here for test and validation ds
test_loss, test_accuracy = model.evaluate(test_ds)
print(f"Test Accuracy: {test_accuracy:.2f}")

val_loss, val_accuracy = model.evaluate(val_ds)
print(f"Validation Accuracy: {val_accuracy:.2f}")

"""# **Visualizing Performance Metrics**"""

# Evaluate on test set and generate predictions
from sklearn.metrics import confusion_matrix
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt # Import the matplotlib library
from sklearn.metrics import classification_report

predictions = model.predict(test_ds)  # Get predictions for test set

# Get true labels and predicted labels
true_labels = np.concatenate([y for x, y in test_ds], axis=0)  # Extract true labels
predicted_labels = np.argmax(predictions, axis=1)  # Get predicted class indices

# Compute confusion matrix
cm = confusion_matrix(y_true=true_labels, y_pred=predicted_labels)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
disp.plot()
plt.grid(False)
plt.title("Test Dataset Confusion Matrix", y=1.04)
plt.show()

# Show Metrics such as Precision, Recall, and F-1 Score
print(classification_report(y_true=true_labels, y_pred=predicted_labels, target_names=class_names))

"""# **Graph Performance Metrics**"""

from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import label_binarize

# Get predicted probabilities using 'predict'
y_probs = model.predict(test_ds)

# Binarize the true labels for multi-class ROC curve
true_labels_binarized = label_binarize(true_labels, classes=class_names)

# Map the labels with encoder
label_encoder = LabelEncoder()
true_labels_encoded = label_encoder.fit_transform(true_labels)

# Binarize true labels for multi-class ROC computation
true_labels_binarized = label_binarize(true_labels_encoded, classes=np.arange(len(class_names)))

# Compute ROC curve and ROC area for each class
fpr = dict() # False Positive Rate
tpr = dict() # True Positive Rate
roc_auc = dict()
# Graphs for every class generated here
for i in range(len(class_names)):
    fpr[i], tpr[i], _ = roc_curve(true_labels_binarized[:, i], y_probs[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Plot ROC curves for each class
plt.figure()
# Labels for every class made here
for i in range(len(class_names)):
    plt.plot(fpr[i], tpr[i], label=f'{class_names[i]} (AUC = {roc_auc[i]:.2f})')
# Plot design here
plt.plot([0, 1], [0, 1], color="navy", linestyle="--")
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Test Dataset ROC Curve (One-vs-Rest)", y=1.04)
plt.legend(loc="lower right")
plt.show()

from sklearn.metrics import precision_recall_curve, auc


# Binarize the true labels for one-vs-rest approach
true_labels_binarized = label_binarize(true_labels, classes=range(len(class_names)))  # Shape: (num_samples, num_classes)

# Precision-Recall curve for each class
precision = dict()
recall = dict()
pr_auc = dict()

plt.figure(figsize=(8, 6))

for i in range(len(class_names)):
    # Compute precision-recall curve and AUC for class 'i'
    precision[i], recall[i], _ = precision_recall_curve(true_labels_binarized[:, i], y_probs[:, i])
    pr_auc[i] = auc(recall[i], precision[i])

    # Plot the precision-recall curve for the current class
    plt.plot(recall[i], precision[i], label=f"{class_names[i]} (AUC = {pr_auc[i]:.2f})")

# Add diagonal reference line
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Test Dataset Precision-Recall Curve (One-vs-Rest)", y=1.04)
plt.legend(loc="lower left")
plt.grid()
plt.show()